{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "High Dimensional BO - GP vs Real Eval learning",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0Gb1BSsWgpZRapR6Yodyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkarsh5k/HighDimBOInLearntSubspace/blob/main/High_Dimensional_BO_GP_vs_Real_Eval_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfaXE2cwhqaA"
      },
      "source": [
        "# High Dimensional Bayesian Optimization using learnt active subspaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcaJMEVIg4Iq",
        "outputId": "b38aa2e7-4e21-454f-fd4f-334d6027d1d8"
      },
      "source": [
        "!pip install torch\n",
        "!pip install gpytorch\n",
        "!pip install botorch\n",
        "!pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.5.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->gpytorch) (3.10.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (1.19.5)\n",
            "Installing collected packages: gpytorch\n",
            "Successfully installed gpytorch-1.5.1\n",
            "Collecting botorch\n",
            "  Downloading botorch-0.5.1-py3-none-any.whl (486 kB)\n",
            "\u001b[K     |████████████████████████████████| 486 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from botorch) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from botorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: gpytorch>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from botorch) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from gpytorch>=1.5.1->botorch) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->botorch) (3.10.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch>=1.5.1->botorch) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch>=1.5.1->botorch) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch>=1.5.1->botorch) (1.19.5)\n",
            "Installing collected packages: botorch\n",
            "Successfully installed botorch-0.5.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUyqDXoKnV-_"
      },
      "source": [
        "import numpy as np \n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.fit import fit_gpytorch_model\n",
        "from botorch.utils import standardize\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from botorch.acquisition import UpperConfidenceBound, qExpectedImprovement\n",
        "from botorch.optim import optimize_acqf\n",
        "from torch import nn as nn\n",
        "import torch \n",
        "from torch.autograd import Variable\n",
        "from torch.quasirandom import SobolEngine\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjKN9Tx8m-iS"
      },
      "source": [
        "def ackley(x): \n",
        "  \"\"\"\n",
        "  Calculates ackley function value for arbitrary number of dimensions\n",
        "  \"\"\"\n",
        "  a = 20 \n",
        "  b = 0.2 \n",
        "  c = 2 * np.pi \n",
        "\n",
        "  n = len(x)\n",
        "  first_operand = -a * np.exp(np.sqrt(np.sum(x**2) / n) * -b)\n",
        "  second_operand = np.exp(np.sum(np.cos(c * x)) / n)\n",
        "\n",
        "  return first_operand - second_operand + a + np.exp(1)\n",
        "\n",
        "offset = [np.random.uniform(low = -10, high = 10) for _ in range(200)]\n",
        "\n",
        "def ackley_prime(x): \n",
        "  assert len(x) == 200\n",
        "  return ackley(x + offset)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x0yPkk3tvgD"
      },
      "source": [
        "class TransformerNetwork(nn.Module):\n",
        "  def __init__(self, original_dim, target_dim):\n",
        "        super().__init__()\n",
        "        self.target_dims = target_dim\n",
        "        self.orig_dims = original_dim\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_nn = nn.Sequential(\n",
        "            nn.Linear(target_dim, target_dim, bias = False),\n",
        "            nn.Relu(),\n",
        "            nn.Linear(target_dim, original_dim, bias = False),\n",
        "        )\n",
        "\n",
        "        self.optimizer = None \n",
        "\n",
        "        self.alpha = 1\n",
        "        self.beta = 1\n",
        "\n",
        "        # [([x_1, x_2...], y)....]\n",
        "        self.ackley_evaluations = []\n",
        "        self.loss_per_step = []\n",
        "        #self.bootstrap_low_dim_space(200)\n",
        "        self.init_optimizer()\n",
        "        self.num_evaluations = 0 \n",
        "        \n",
        "  def init_optimizer(self):\n",
        "    learning_rate = 1e-3\n",
        "    self.optimizer = torch.optim.SGD(self.linear_nn.parameters(), lr=learning_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = self.linear_nn(x)\n",
        "    return logits\n",
        "\n",
        "  def get_transformation_loss(self, points):\n",
        "    '''\n",
        "    Gets average euclidean distance between original point space\n",
        "    vs same points reconstructed through one forward and reverse \n",
        "    pass through the neural network\n",
        "    '''\n",
        "    forward_reverse_transformation = self.reverse_pass(self.forward(points).detach().numpy())\n",
        "    diff_mat = points.detach().numpy() - forward_reverse_transformation\n",
        "    return np.sqrt(np.sum(diff_mat**2)) / points.shape[0]  \n",
        "\n",
        "  def reverse_pass(self, x):     \n",
        "    weight_copy = self.linear_nn[0].weight.detach().numpy()\n",
        "\n",
        "    low_dim = x \n",
        "    for i in range(len(self.linear_nn) - 1, -1, -1):\n",
        "      reverse_transformation = np.linalg.pinv(self.linear_nn[i].weight.detach().numpy().T)\n",
        "      low_dim = np.matmul(low_dim, reverse_transformation)\n",
        "    return low_dim\n",
        "  \n",
        "  def train_loop(self):\n",
        "    for i in range(100):\n",
        "      candidate, prediction = self.run_bayes_op()\n",
        "      loss = Variable(self.get_full_loss(candidate, prediction), requires_grad = True)\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "  def project_high_dim_history_to_low(self): \n",
        "    eval_x = np.array([x[0] for x in self.ackley_evaluations])\n",
        "    low_dim_eval_x = self.reverse_pass(eval_x)\n",
        "    return torch.tensor(low_dim_eval_x), torch.tensor([[x[1] * -1] for x in self.ackley_evaluations])\n",
        "\n",
        "  def get_evals_for_low_dim(self, low_dim): \n",
        "    eval_val = []\n",
        "    eval_x = []\n",
        "    for pt in low_dim: \n",
        "      x = self.forward(torch.tensor(pt.detach().numpy(), dtype = torch.float)).detach().numpy()\n",
        "      eval_x.append(pt.detach().numpy())   \n",
        "      eval_val.append(ackley_prime(x))\n",
        "    #print(eval_x)\n",
        "    return torch.tensor(eval_x, dtype = torch.float), torch.tensor([[x * -1] for x in eval_val])\n",
        "\n",
        "  def get_full_loss(self, point, prediction): \n",
        "    high_dim_point = self.forward(point).detach().numpy()\n",
        "    high_dim_eval = ackley_prime(high_dim_point)\n",
        "    self.ackley_evaluations.append((point.detach().numpy(), high_dim_eval))\n",
        "    loss = self.alpha * (high_dim_eval - prediction)\n",
        "    loss = loss + self.beta * (self.get_transformation_loss(point)) \n",
        "    print(f'High dimensional eval: {high_dim_eval} Prediction: {prediction} loss = {loss}')\n",
        "    return torch.tensor([loss])\n",
        "\n",
        "  def bootstrap_low_dim_space(self, num_points = 1): \n",
        "    '''\n",
        "    We need to have at least one sample in the low dim space to run BayesOP\n",
        "    '''\n",
        "    evals = []\n",
        "\n",
        "    points = self.get_initial_points(self.orig_dims, num_points).detach().numpy()\n",
        "    #points = self.latin_hypercube(num_points, self.orig_dims)\n",
        "\n",
        "    for point in points:\n",
        "      #sample = np.array([np.random.uniform(low = -32.768, high = 32.768) for _ in range(self.orig_dims)])\n",
        "      eval = ackley_prime(point)\n",
        "      evals.append((point, eval))\n",
        "    \n",
        "    return evals\n",
        "\n",
        "  def get_initial_points(self, dim, n_pts, seed=0):\n",
        "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
        "    X_init = sobol.draw(n=n_pts).to(dtype=float)\n",
        "    return X_init\n",
        "    \n",
        "  def latin_hypercube(self, n_pts, dim):\n",
        "    \"\"\"Basic Latin hypercube implementation with center perturbation.\"\"\"\n",
        "    X = np.zeros((n_pts, dim))\n",
        "    centers = (1.0 + 2.0 * np.arange(0.0, n_pts)) / float(2 * n_pts)\n",
        "    for i in range(dim):  # Shuffle the center locataions for each dimension.\n",
        "        X[:, i] = centers[np.random.permutation(n_pts)]\n",
        "\n",
        "    # Add some perturbations within each box\n",
        "    pert = np.random.uniform(-1.0, 1.0, (n_pts, dim)) / float(2 * n_pts)\n",
        "    X += pert\n",
        "    return X\n",
        "\n",
        "  def run_bayes_op(self):   \n",
        "    init_points = self.get_initial_points(self.target_dims, 50)\n",
        "    train_X, train_Y = self.get_evals_for_low_dim(init_points)\n",
        "\n",
        "    gp = SingleTaskGP(train_X, train_Y)\n",
        "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "    fit_gpytorch_model(mll)\n",
        "\n",
        "    # Optimize acquisition function \n",
        "    #UCB = UpperConfidenceBound(gp, beta=0.1)\n",
        "    ei = qExpectedImprovement(gp, train_Y.max(), maximize=True)\n",
        "    #bounds = torch.stack([torch.tensor([0] * self.target_dims), torch.tensor([1] * self.target_dims)])\n",
        "    bounds = torch.stack([torch.tensor([-32] * self.target_dims, dtype = torch.float), torch.tensor([32] * self.target_dims, dtype = torch.float)])\n",
        "    candidate, acq_value = optimize_acqf(ei, bounds=bounds, q=1, num_restarts=10, raw_samples=512)\n",
        "    prediction = gp.posterior(candidate).mean.detach().numpy()[0, 0]\n",
        "    best_f = train_Y.min()\n",
        "    print(f\"Expected Improvement: {ei(train_X)}\")\n",
        "    return candidate[0], prediction * -1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN8zfMFrps5h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "1d193099-5d30-4faf-b16d-0740ce73b8b1"
      },
      "source": [
        "network = TransformerNetwork(original_dim = 200, target_dim = 10)\n",
        "#network = SVDBO(original_dim = 200, target_dim = 20)\n",
        "network.train_loop()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected Improvement: tensor([0.0025], grad_fn=<MeanBackward1>)\n",
            "High dimensional eval: 19.832565109352952 Prediction: 15.450535774230957 loss = 4.38203021177648\n",
            "Expected Improvement: tensor([0.0025], grad_fn=<MeanBackward1>)\n",
            "High dimensional eval: 19.845438830105053 Prediction: 15.450535774230957 loss = 4.394903953037868\n",
            "Expected Improvement: tensor([0.0025], grad_fn=<MeanBackward1>)\n",
            "High dimensional eval: 20.425362422916443 Prediction: 15.450535774230957 loss = 4.974827297254161\n",
            "Expected Improvement: tensor([0.0025], grad_fn=<MeanBackward1>)\n",
            "High dimensional eval: 20.222247072325395 Prediction: 15.450535774230957 loss = 4.771712473923567\n",
            "Expected Improvement: tensor([0.0025], grad_fn=<MeanBackward1>)\n",
            "High dimensional eval: 20.576781762284163 Prediction: 15.450535774230957 loss = 5.126246381669553\n",
            "Expected Improvement: tensor([0.0026], grad_fn=<MeanBackward1>)\n",
            "High dimensional eval: 20.123439157152404 Prediction: 15.450535774230957 loss = 4.672904427687297\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c2e828b4692a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#network = SVDBO(original_dim = 200, target_dim = 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-cfe50a5c70bd>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m       \u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_bayes_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_full_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-cfe50a5c70bd>\u001b[0m in \u001b[0;36mrun_bayes_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleTaskGP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mmll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExactMarginalLogLikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mfit_gpytorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Optimize acquisition function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botorch/fit.py\u001b[0m in \u001b[0;36mfit_gpytorch_model\u001b[0;34m(mll, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mmll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0msample_all_priors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mhas_optwarning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botorch/optim/fit.py\u001b[0m in \u001b[0;36mfit_gpytorch_scipy\u001b[0;34m(mll, bounds, method, options, track_iterations, approx_mll, scipy_objective, module_to_array_func, module_from_array_func)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botorch/optim/utils.py\u001b[0m in \u001b[0;36m_scipy_objective_and_grad\u001b[0;34m(x, mll, property_dict)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mparam_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOWphT0e8TfL"
      },
      "source": [
        "evaluations = [x[1] for x in network.ackley_evaluations]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAVEv-Pz8Tei"
      },
      "source": [
        "monotonic_evaluations = []\n",
        "min = evaluations[0]\n",
        "for x in evaluations: \n",
        "  if x < min: \n",
        "    min = x\n",
        "  \n",
        "  monotonic_evaluations.append(min)\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "x_axis = [i for i in range(1, len(evaluations) + 1)]\n",
        "plt.plot(x_axis, monotonic_evaluations)\n",
        "plt.xlabel(\"Number of evaluations\")\n",
        "plt.ylabel(\"200D Ackley Function Value\")\n",
        "plt.title(\"Number of evaluations vs smallest 200D Ackley Fn Value\")\n",
        "\n",
        "from google.colab import files\n",
        "plt.savefig(\"200DAckley.png\")\n",
        "files.download(\"200DAckley.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO-GWqthUWWp"
      },
      "source": [
        "class SVDBO(): \n",
        "  def __init__(self, original_dim, target_dim): \n",
        "    self.target_dims = target_dim\n",
        "    self.orig_dims = original_dim\n",
        "\n",
        "    self.alpha = 1\n",
        "    self.beta = 1\n",
        "\n",
        "    # [([x_1, x_2...], y)....]\n",
        "    self.ackley_evaluations = []\n",
        "    self.num_evaluations = 0 \n",
        "    self.initial_points  = self.latin_hypercube(self.orig_dims, self.orig_dims)\n",
        "    self.evaluations = []\n",
        "    self.get_evaluations()\n",
        "    self.points_mean = None\n",
        "    _, _, self.right = self.perform_svd()\n",
        "    self.subspace_axes = self.right[0: self.target_dims]\n",
        "  \n",
        "  def latin_hypercube(self, n_pts, dim):\n",
        "    \"\"\"Basic Latin hypercube implementation with center perturbation.\"\"\"\n",
        "    X = np.zeros((n_pts, dim))\n",
        "    centers = (1.0 + 2.0 * np.arange(0.0, n_pts)) / float(2 * n_pts)\n",
        "    for i in range(dim):  # Shuffle the center locataions for each dimension.\n",
        "        X[:, i] = centers[np.random.permutation(n_pts)]\n",
        "\n",
        "    # Add some perturbations within each box\n",
        "    pert = np.random.uniform(-1.0, 1.0, (n_pts, dim)) / float(2 * n_pts)\n",
        "    X += pert\n",
        "    return X\n",
        "\n",
        "  def project_points_into_subspace(self, points):\n",
        "    return points @ self.subspace_axes.T\n",
        "  \n",
        "  def get_reverse_projection(self, points):\n",
        "    return (points @ self.subspace_axes) + self.points_mean\n",
        "\n",
        "  def perform_svd(self):\n",
        "    self.points_mean = self.initial_points.mean(axis = 0)\n",
        "    return np.linalg.svd(self.initial_points - self.points_mean)\n",
        "\n",
        "  def get_evaluations(self):\n",
        "    for pt in self.initial_points:\n",
        "      self.evaluations.append(ackley_prime(pt)) \n",
        "\n",
        "  def get_evals_for_low_dim(self):\n",
        "    low_dim_points = self.project_points_into_subspace(self.initial_points)\n",
        "    return torch.tensor(low_dim_points, dtype = torch.float), torch.tensor([[x * -1] for x in self.evaluations])\n",
        "\n",
        "  def train_loop(self):\n",
        "    for _ in range(500):\n",
        "      candidate, prediction = self.run_bayes_op()\n",
        "      high_dim = self.get_reverse_projection(np.array([candidate.detach().numpy()]))[0]\n",
        "      self.initial_points = np.vstack([self.initial_points, high_dim])\n",
        "      eval = ackley_prime(high_dim)\n",
        "      self.evaluations.append(eval)\n",
        "      print(f\"High Dim Actual: {eval} GP Predicted: {prediction}\")    \n",
        "\n",
        "  def run_bayes_op(self):   \n",
        "    train_X, train_Y = self.get_evals_for_low_dim()\n",
        "\n",
        "    gp = SingleTaskGP(train_X, train_Y)\n",
        "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "    fit_gpytorch_model(mll)\n",
        "\n",
        "    # Optimize acquisition function \n",
        "    #UCB = UpperConfidenceBound(gp, beta=0.1)\n",
        "    ei = qExpectedImprovement(gp, train_Y.max(), maximize=True)\n",
        "    #bounds = torch.stack([torch.tensor([0] * self.target_dims), torch.tensor([1] * self.target_dims)])\n",
        "    bounds = torch.stack([torch.tensor([-32] * self.target_dims, dtype = torch.float), torch.tensor([32] * self.target_dims, dtype = torch.float)])\n",
        "    candidate, acq_value = optimize_acqf(ei, bounds=bounds, q=1, num_restarts=10, raw_samples=512)\n",
        "    prediction = gp.posterior(candidate).mean.detach().numpy()[0, 0]\n",
        "    return candidate[0], prediction * -1 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}