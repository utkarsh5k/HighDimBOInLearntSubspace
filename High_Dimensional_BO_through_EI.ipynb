{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "High Dimensional BO through EI",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkUVEpWJ0icSQs0fPN52Ph",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkarsh5k/HighDimBOInLearntSubspace/blob/main/High_Dimensional_BO_through_EI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfaXE2cwhqaA"
      },
      "source": [
        "# High Dimensional Bayesian Optimization using learnt active subspaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcaJMEVIg4Iq",
        "outputId": "08956968-33d4-4c38-d80b-78924acdbb84"
      },
      "source": [
        "!pip install torch\n",
        "!pip install gpytorch\n",
        "!pip install botorch\n",
        "!pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.5.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.0.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->gpytorch) (3.10.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (1.19.5)\n",
            "Installing collected packages: gpytorch\n",
            "Successfully installed gpytorch-1.5.1\n",
            "Collecting botorch\n",
            "  Downloading botorch-0.5.1-py3-none-any.whl (486 kB)\n",
            "\u001b[K     |████████████████████████████████| 486 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from botorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: gpytorch>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from botorch) (1.5.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from botorch) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from gpytorch>=1.5.1->botorch) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->botorch) (3.10.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch>=1.5.1->botorch) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch>=1.5.1->botorch) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch>=1.5.1->botorch) (1.19.5)\n",
            "Installing collected packages: botorch\n",
            "Successfully installed botorch-0.5.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUyqDXoKnV-_"
      },
      "source": [
        "import numpy as np \n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.fit import fit_gpytorch_model\n",
        "from botorch.utils import standardize\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from botorch.acquisition import UpperConfidenceBound, qExpectedImprovement\n",
        "from botorch.optim import optimize_acqf\n",
        "from torch import nn as nn\n",
        "import torch \n",
        "from torch.autograd import Variable\n",
        "from torch.quasirandom import SobolEngine\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjKN9Tx8m-iS"
      },
      "source": [
        "def ackley(x): \n",
        "  \"\"\"\n",
        "  Calculates ackley function value for arbitrary number of dimensions\n",
        "  \"\"\"\n",
        "  a = 20 \n",
        "  b = 0.2 \n",
        "  c = 2 * np.pi \n",
        "\n",
        "  n = len(x)\n",
        "  first_operand = -a * np.exp(np.sqrt(np.sum(x**2) / n) * -b)\n",
        "  second_operand = np.exp(np.sum(np.cos(c * x)) / n)\n",
        "\n",
        "  return first_operand - second_operand + a + np.exp(1)\n",
        "\n",
        "offset = [np.random.uniform(low = -10, high = 10) for _ in range(200)]\n",
        "\n",
        "def ackley_prime(x): \n",
        "  assert len(x) == 200\n",
        "  return ackley(x + offset)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x0yPkk3tvgD"
      },
      "source": [
        "class TransformerNetwork(nn.Module):\n",
        "  def __init__(self, original_dim, target_dim):\n",
        "        super().__init__()\n",
        "        self.target_dims = target_dim\n",
        "        self.orig_dims = original_dim\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_nn = nn.Sequential(\n",
        "            nn.Linear(target_dim, original_dim, bias = False),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.optimizer = None \n",
        "\n",
        "        self.alpha = 1\n",
        "        self.beta = 1\n",
        "\n",
        "        # [([x_1, x_2...], y)....]\n",
        "        self.ackley_evaluations = []\n",
        "        self.loss_per_step = []\n",
        "        self.bootstrap_low_dim_space(200)\n",
        "        self.init_optimizer()\n",
        "        self.num_evaluations = 0\n",
        "\n",
        "\n",
        "\n",
        "  def init_optimizer(self):\n",
        "    learning_rate = 1e-3\n",
        "    self.optimizer = torch.optim.SGD(self.linear_nn.parameters(), lr=learning_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = self.linear_nn(x)\n",
        "    return logits\n",
        "  \n",
        "  def train_loop(self):\n",
        "    for i in range(100):\n",
        "      candidate, prediction, ei = self.run_bayes_op()\n",
        "      loss = Variable(self.get_ei_loss(candidate, ei.detach().numpy()[0]), requires_grad = True)\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "  def get_low_dim_evaluations(self): \n",
        "     return torch.tensor([x[0] for x in self.ackley_evaluations]), torch.tensor([[x[1] * -1] for x in self.ackley_evaluations])\n",
        "\n",
        "  def get_evals_for_low_dim(self, low_dim): \n",
        "    eval_val = []\n",
        "    eval_x = []\n",
        "    for pt in low_dim: \n",
        "      x = self.forward(torch.tensor(pt.detach().numpy(), dtype = torch.float)).detach().numpy()\n",
        "      eval_x.append(pt.detach().numpy())   \n",
        "      eval_val.append(ackley_prime(x))\n",
        "    #print(eval_x)\n",
        "    return torch.tensor(eval_x, dtype = torch.float), torch.tensor([[x * -1] for x in eval_val])\n",
        "\n",
        "  def get_full_loss(self, point, prediction): \n",
        "    high_dim_point = self.forward(point).detach().numpy()\n",
        "    high_dim_eval = ackley_prime(high_dim_point)\n",
        "    \n",
        "\n",
        "    self.ackley_evaluations.append((point.detach().numpy(), high_dim_eval))\n",
        "    loss = self.alpha * (high_dim_eval - prediction)\n",
        "    print(f'High dimensional eval: {high_dim_eval} Prediction: {prediction} loss = {loss}')\n",
        "    return torch.tensor([loss])\n",
        "\n",
        "  def get_ei_loss(self, pred_point, pred_ei):\n",
        "    high_dim_point = self.forward(pred_point).detach().numpy()\n",
        "    high_dim_eval = ackley_prime(high_dim_point)\n",
        "    self.num_evaluations += 1\n",
        "    cur_min_eval = self.get_min_eval()\n",
        "    self.ackley_evaluations.append((pred_point.detach().numpy(), high_dim_eval))\n",
        "    \n",
        "    actual_imp = cur_min_eval - high_dim_eval\n",
        "    loss = (actual_imp - pred_ei) * -1 \n",
        "    if loss < 0: \n",
        "      loss = 0\n",
        "    print(f'[{self.num_evaluations}] High dimensional eval: {high_dim_eval} loss = {loss}')\n",
        "    return torch.tensor([loss * 100])\n",
        "\n",
        "  def get_min_eval(self):\n",
        "    return np.array([x[1] for x in self.ackley_evaluations]).min()\n",
        "\n",
        "  def bootstrap_low_dim_space(self, num_points = 1): \n",
        "    '''\n",
        "    We need to have at least one sample in the low dim space to run BayesOP\n",
        "    '''\n",
        "    \n",
        "    points = self.get_initial_points(self.target_dims, num_points).detach().numpy()\n",
        "    #points = self.latin_hypercube(num_points, self.orig_dims)\n",
        "\n",
        "    for point in points:\n",
        "      high_dim_point = self.forward(torch.tensor(point, dtype = torch.float)).detach().numpy()\n",
        "      eval = ackley_prime(high_dim_point)\n",
        "      self.ackley_evaluations.append((point, eval))\n",
        "    \n",
        "\n",
        "  def get_initial_points(self, dim, n_pts, seed=0):\n",
        "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
        "    X_init = sobol.draw(n=n_pts).to(dtype=float)\n",
        "    return X_init\n",
        "    \n",
        "  def latin_hypercube(self, n_pts, dim):\n",
        "    \"\"\"Basic Latin hypercube implementation with center perturbation.\"\"\"\n",
        "    X = np.zeros((n_pts, dim))\n",
        "    centers = (1.0 + 2.0 * np.arange(0.0, n_pts)) / float(2 * n_pts)\n",
        "    for i in range(dim):  # Shuffle the center locataions for each dimension.\n",
        "        X[:, i] = centers[np.random.permutation(n_pts)]\n",
        "\n",
        "    # Add some perturbations within each box\n",
        "    pert = np.random.uniform(-1.0, 1.0, (n_pts, dim)) / float(2 * n_pts)\n",
        "    X += pert\n",
        "    return X\n",
        "\n",
        "  def run_bayes_op(self):   \n",
        "    train_X, train_Y = self.get_low_dim_evaluations()\n",
        "\n",
        "    gp = SingleTaskGP(train_X, train_Y)\n",
        "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "    fit_gpytorch_model(mll)\n",
        "\n",
        "    # Optimize acquisition function \n",
        "    #UCB = UpperConfidenceBound(gp, beta=0.1)\n",
        "    ei = qExpectedImprovement(gp, train_Y.max(), maximize=True)\n",
        "    #bounds = torch.stack([torch.tensor([0] * self.target_dims), torch.tensor([1] * self.target_dims)])\n",
        "    bounds = torch.stack([torch.tensor([-32] * self.target_dims, dtype = torch.float), torch.tensor([32] * self.target_dims, dtype = torch.float)])\n",
        "    candidate, acq_value = optimize_acqf(ei, bounds=bounds, q=1, num_restarts=10, raw_samples=512)\n",
        "    prediction = gp.posterior(candidate).mean.detach().numpy()[0, 0]\n",
        "    best_f = train_Y.min()\n",
        "    return candidate[0], prediction * -1, ei(train_X)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN8zfMFrps5h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f16c7bbf-0137-4b99-d255-0aae92abe1b8"
      },
      "source": [
        "network = TransformerNetwork(original_dim = 200, target_dim = 20)\n",
        "#network = SVDBO(original_dim = 200, target_dim = 20)\n",
        "network.train_loop()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] High dimensional eval: 18.431241719860946 loss = 3.367053574349297\n",
            "[2] High dimensional eval: 17.376715955550463 loss = 2.317981651261713\n",
            "[3] High dimensional eval: 18.453580141231818 loss = 3.3944931003678374\n",
            "[4] High dimensional eval: 18.089871568541415 loss = 3.0310159470015297\n",
            "[5] High dimensional eval: 18.210687951740823 loss = 3.151925667881037\n",
            "[6] High dimensional eval: 18.212582454805528 loss = 3.1536691051249224\n",
            "[7] High dimensional eval: 18.47487475195973 loss = 3.4160384289399386\n",
            "[8] High dimensional eval: 17.022112474704183 loss = 1.9633782169739957\n",
            "[9] High dimensional eval: 16.624983624550115 loss = 1.6438945563997727\n",
            "[10] High dimensional eval: 19.529324897127378 loss = 4.470491016643217\n",
            "[11] High dimensional eval: 18.565976200148466 loss = 3.507061338450494\n",
            "[12] High dimensional eval: 17.311434724131765 loss = 2.252959316389744\n",
            "[13] High dimensional eval: 18.295941787536673 loss = 3.237127951521929\n",
            "[14] High dimensional eval: 17.52781534491656 loss = 2.4689868478080075\n",
            "[15] High dimensional eval: 18.38493586628327 loss = 3.3261210904802665\n",
            "[16] High dimensional eval: 18.51996991198718 loss = 3.4614245646011303\n",
            "[17] High dimensional eval: 18.508342741283 loss = 3.4495095224450547\n",
            "[18] High dimensional eval: 18.126028447788194 loss = 3.067620525482819\n",
            "[19] High dimensional eval: 18.274414151322276 loss = 3.2157813613014388\n",
            "[20] High dimensional eval: 17.504919739614134 loss = 2.4463161519652816\n",
            "[21] High dimensional eval: 18.677861851876305 loss = 3.6190487642742326\n",
            "[22] High dimensional eval: 19.103646617482127 loss = 4.045115758572262\n",
            "[23] High dimensional eval: 17.67819256489541 loss = 2.6195145502033603\n",
            "[24] High dimensional eval: 18.49608364264338 loss = 3.4372371817126903\n",
            "[25] High dimensional eval: 17.657528941404145 loss = 2.598933344213624\n",
            "[26] High dimensional eval: 18.58868402165384 loss = 3.529891104338375\n",
            "[27] High dimensional eval: 18.007427603916433 loss = 2.948894061974908\n",
            "[28] High dimensional eval: 17.63786605963268 loss = 2.579466063685351\n",
            "[29] High dimensional eval: 18.460986789756966 loss = 3.402245011385039\n",
            "[30] High dimensional eval: 18.99691850511369 loss = 3.938381161568911\n",
            "[31] High dimensional eval: 19.485811542066095 loss = 4.4270432989706645\n",
            "[32] High dimensional eval: 17.608126645048415 loss = 2.5494634569139643\n",
            "[33] High dimensional eval: 17.768914603891325 loss = 2.710444599756374\n",
            "[34] High dimensional eval: 18.02569018489988 loss = 2.9669505772061373\n",
            "[35] High dimensional eval: 17.70072712950672 loss = 2.6419452981105085\n",
            "[36] High dimensional eval: 17.722313127720522 loss = 2.663601285668075\n",
            "[37] High dimensional eval: 18.019513499066303 loss = 2.9610648561281505\n",
            "[38] High dimensional eval: 18.127924994011085 loss = 3.0693880178408377\n",
            "[39] High dimensional eval: 18.836057626351582 loss = 3.7774212108502914\n",
            "[40] High dimensional eval: 17.796009128110946 loss = 3.327717087818716\n",
            "[41] High dimensional eval: 18.666981833049284 loss = 3.608469466433837\n",
            "[42] High dimensional eval: 18.38856668652118 loss = 3.3301790917223495\n",
            "[43] High dimensional eval: 18.078514829003826 loss = 3.0199769472247175\n",
            "[44] High dimensional eval: 17.81290981781732 loss = 2.7541486583085377\n",
            "[45] High dimensional eval: 18.90161286540118 loss = 3.8428646299670937\n",
            "[46] High dimensional eval: 18.08543367539179 loss = 3.0268734622483615\n",
            "[47] High dimensional eval: 17.657760736610864 loss = 2.599244900607994\n",
            "[48] High dimensional eval: 18.42114970201095 loss = 3.362679327238793\n",
            "[49] High dimensional eval: 18.356454835389087 loss = 3.2979091322671\n",
            "[50] High dimensional eval: 17.921826208952286 loss = 2.8630092451458773\n",
            "[51] High dimensional eval: 18.673031670930865 loss = 3.6144551476825533\n",
            "[52] High dimensional eval: 17.733065426331237 loss = 2.6743068838395905\n",
            "[53] High dimensional eval: 19.330285803160542 loss = 4.271518711061447\n",
            "[54] High dimensional eval: 17.983597348125787 loss = 2.92493071045957\n",
            "[55] High dimensional eval: 19.023782695055075 loss = 3.965777382857295\n",
            "[56] High dimensional eval: 17.985684904914965 loss = 2.927058687139006\n",
            "[57] High dimensional eval: 18.428692877023337 loss = 3.3698801578660706\n",
            "[58] High dimensional eval: 17.513481511537375 loss = 2.4550095194616777\n",
            "[59] High dimensional eval: 18.924360212689404 loss = 3.8656954209398124\n",
            "[60] High dimensional eval: 17.772232330037294 loss = 2.7136732879249488\n",
            "[61] High dimensional eval: 18.75958200470261 loss = 3.700797609852671\n",
            "[62] High dimensional eval: 18.466712201743896 loss = 3.4083954843471265\n",
            "[63] High dimensional eval: 18.66181176035126 loss = 3.603113607284906\n",
            "[64] High dimensional eval: 19.175551119076424 loss = 4.11678127011585\n",
            "[65] High dimensional eval: 17.908238259021022 loss = 2.849821331570349\n",
            "[66] High dimensional eval: 17.688125922666043 loss = 2.629375406215915\n",
            "[67] High dimensional eval: 17.932930335558073 loss = 2.874450098723408\n",
            "[68] High dimensional eval: 17.683126427597188 loss = 2.62451827235756\n",
            "[69] High dimensional eval: 17.64746400607394 loss = 2.588697040539179\n",
            "[70] High dimensional eval: 18.531851533748515 loss = 3.4731685656596407\n",
            "[71] High dimensional eval: 18.15459128842475 loss = 3.09593284820461\n",
            "[72] High dimensional eval: 18.4273969467718 loss = 3.368930616113495\n",
            "[73] High dimensional eval: 19.183961835938685 loss = 4.125311066120317\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-758b77a700d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#network = SVDBO(original_dim = 200, target_dim = 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-8799de08e197>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_bayes_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ei_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-8799de08e197>\u001b[0m in \u001b[0;36mrun_bayes_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleTaskGP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExactMarginalLogLikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mfit_gpytorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# Optimize acquisition function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botorch/fit.py\u001b[0m in \u001b[0;36mfit_gpytorch_model\u001b[0;34m(mll, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mmll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0msample_all_priors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mhas_optwarning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botorch/optim/fit.py\u001b[0m in \u001b[0;36mfit_gpytorch_scipy\u001b[0;34m(mll, bounds, method, options, track_iterations, approx_mll, scipy_objective, module_to_array_func, module_from_array_func)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botorch/optim/utils.py\u001b[0m in \u001b[0;36m_scipy_objective_and_grad\u001b[0;34m(x, mll, property_dict)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mparam_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOWphT0e8TfL"
      },
      "source": [
        "evaluations = [x[1] for x in network.ackley_evaluations]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAVEv-Pz8Tei"
      },
      "source": [
        "monotonic_evaluations = []\n",
        "min = evaluations[0]\n",
        "for x in evaluations: \n",
        "  if x < min: \n",
        "    min = x\n",
        "  \n",
        "  monotonic_evaluations.append(min)\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "x_axis = [i for i in range(1, len(evaluations) + 1)]\n",
        "plt.plot(x_axis, monotonic_evaluations)\n",
        "plt.xlabel(\"Number of evaluations\")\n",
        "plt.ylabel(\"200D Ackley Function Value\")\n",
        "plt.title(\"Number of evaluations vs smallest 200D Ackley Fn Value\")\n",
        "\n",
        "from google.colab import files\n",
        "plt.savefig(\"200DAckley.png\")\n",
        "files.download(\"200DAckley.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO-GWqthUWWp"
      },
      "source": [
        "class SVDBO(): \n",
        "  def __init__(self, original_dim, target_dim): \n",
        "    self.target_dims = target_dim\n",
        "    self.orig_dims = original_dim\n",
        "\n",
        "    self.alpha = 1\n",
        "    self.beta = 1\n",
        "\n",
        "    # [([x_1, x_2...], y)....]\n",
        "    self.ackley_evaluations = []\n",
        "    self.num_evaluations = 0 \n",
        "    self.initial_points  = self.latin_hypercube(self.orig_dims, self.orig_dims)\n",
        "    self.evaluations = []\n",
        "    self.get_evaluations()\n",
        "    self.points_mean = None\n",
        "    _, _, self.right = self.perform_svd()\n",
        "    self.subspace_axes = self.right[0: self.target_dims]\n",
        "  \n",
        "  def latin_hypercube(self, n_pts, dim):\n",
        "    \"\"\"Basic Latin hypercube implementation with center perturbation.\"\"\"\n",
        "    X = np.zeros((n_pts, dim))\n",
        "    centers = (1.0 + 2.0 * np.arange(0.0, n_pts)) / float(2 * n_pts)\n",
        "    for i in range(dim):  # Shuffle the center locataions for each dimension.\n",
        "        X[:, i] = centers[np.random.permutation(n_pts)]\n",
        "\n",
        "    # Add some perturbations within each box\n",
        "    pert = np.random.uniform(-1.0, 1.0, (n_pts, dim)) / float(2 * n_pts)\n",
        "    X += pert\n",
        "    return X\n",
        "\n",
        "  def project_points_into_subspace(self, points):\n",
        "    return points @ self.subspace_axes.T\n",
        "  \n",
        "  def get_reverse_projection(self, points):\n",
        "    return (points @ self.subspace_axes) + self.points_mean\n",
        "\n",
        "  def perform_svd(self):\n",
        "    self.points_mean = self.initial_points.mean(axis = 0)\n",
        "    return np.linalg.svd(self.initial_points - self.points_mean)\n",
        "\n",
        "  def get_evaluations(self):\n",
        "    for pt in self.initial_points:\n",
        "      self.evaluations.append(ackley_prime(pt)) \n",
        "\n",
        "  def get_evals_for_low_dim(self):\n",
        "    low_dim_points = self.project_points_into_subspace(self.initial_points)\n",
        "    return torch.tensor(low_dim_points, dtype = torch.float), torch.tensor([[x * -1] for x in self.evaluations])\n",
        "\n",
        "  def train_loop(self):\n",
        "    for _ in range(500):\n",
        "      candidate, prediction = self.run_bayes_op()\n",
        "      high_dim = self.get_reverse_projection(np.array([candidate.detach().numpy()]))[0]\n",
        "      self.initial_points = np.vstack([self.initial_points, high_dim])\n",
        "      eval = ackley_prime(high_dim)\n",
        "      self.evaluations.append(eval)\n",
        "      print(f\"High Dim Actual: {eval} GP Predicted: {prediction}\")    \n",
        "\n",
        "  def run_bayes_op(self):   \n",
        "    train_X, train_Y = self.get_low_dim_evaluations()\n",
        "\n",
        "    gp = SingleTaskGP(train_X, train_Y)\n",
        "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "    fit_gpytorch_model(mll)\n",
        "\n",
        "    # Optimize acquisition function \n",
        "    #UCB = UpperConfidenceBound(gp, beta=0.1)\n",
        "    ei = qExpectedImprovement(gp, train_Y.max(), maximize=True)\n",
        "    #bounds = torch.stack([torch.tensor([0] * self.target_dims), torch.tensor([1] * self.target_dims)])\n",
        "    bounds = torch.stack([torch.tensor([-32] * self.target_dims, dtype = torch.float), torch.tensor([32] * self.target_dims, dtype = torch.float)])\n",
        "    candidate, acq_value = optimize_acqf(ei, bounds=bounds, q=1, num_restarts=10, raw_samples=512)\n",
        "    prediction = gp.posterior(candidate).mean.detach().numpy()[0, 0]\n",
        "    return candidate[0], prediction * -1 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}